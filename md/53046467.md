转载请注明出处：

[http://blog.csdn.net/gane_cheng/article/details/53046467](http://blog.csdn.net/gane_cheng/article/details/53046467)

[http://www.ganecheng.tech/blog/53046467.html](http://www.ganecheng.tech/blog/53046467.html) （浏览效果更好）

事情的起因是这样的，无意中看到排行榜首的一篇文章，文章只有一句话，但是访问量却有六万多。

![这里写图片描述](http://img.blog.csdn.net/20161105173258633)

![这里写图片描述](http://img.blog.csdn.net/20161105173316337)

我也有几篇文章曾经被推荐到CSDN博客首页，但是累积下来一篇也才3000左右。

毫无疑问，没有任何营养的文章短时间内访问量却这么高，肯定是通过某种方式刷出来的。

正好 [yixianfeng41](http://blog.csdn.net/yixianfeng41) 大神做过相关方面的研究，在大神的指导下，我总结了这个过程。

**概念简介**
====

先正三观，确保我们研究的是同一个东西。

**CSDN博客访问量**：某一篇文章被所有独立ip访问的次数的总和。具体到某一个ip来说，在一段时间内的访问只统计一次。不管用户有没有登录，只要访问了博客的链接，就会被CSDN统计。访问次数每超过100次：可获得1分，阅读加分最高加到100分，即文章点击上万次截止。

具体的规则可以参考此链接[http://blog.csdn.net/csdnproduct/article/details/6603568](http://blog.csdn.net/csdnproduct/article/details/6603568)

**刷访问量**：为了满足数字上的虚荣，而通过某种方式来破坏规则，有目的地去改变正常的结果。


**原理分析**
====

通过上面的概念简介，我们可以知道，一个ip在一段时间内打开一个链接就会被统计一次。如果，我们用程序去循环访问这个链接，比如一个小时访问一次，意义也不大，远没有访问量在短时间内暴增的感觉。

但是，我们如果使用代理ip来执行我们的请求，CSDN获取到的客户端ip就是代理ip，这样一段时间内代理ip的访问就会被统计一次。

**HTTP代理原理**

![这里写图片描述](http://img.blog.csdn.net/20161105192359057)

如果我们有六万个代理IP，每个代理IP访问一次，即可实现短时间内访问量破六万的效果。

下面详细介绍这个过程。


**实现过程**
====

下面将按照这样一个流程来实现这个过程。先找出需要刷的博客文章页面，然后寻找免费的代理IP，验证代理IP是否有效，刷博客文章访问量。

查找需要刷的博客文章网址
------------

我们的博客所有文章，一般在博客列表页面。

[http://blog.csdn.net/gane_cheng/article/list/1](http://blog.csdn.net/gane_cheng/article/list/1)

不同的是中间的博客名称`gane_cheng`，以及页码`1`。

再来看页面链接的格式。

![这里写图片描述](http://img.blog.csdn.net/20161105201240875)

[/gane_cheng/article/details/53001846](/gane_cheng/article/details/53001846)

其中，`gane_cheng`是博客名称，`53001846`则是博客文章的id。

有了这些信息，我们就可以找出一个博客下面所有的博客文章链接。

```
	/**
	 * 获取所有需要刷访问量的博客的链接
	 * 
	 * @param blogName
	 * @throws IOException
	 */
	public static Map<String, String> getBlogLinkList(String blogName) throws IOException
	{

		Map<String, String> blogMap = new HashMap<String, String>();
		
		for (int index = 1; index < 100; index++)
		{
			URL obj = new URL("http://blog.csdn.net/" + blogName + "/article/list/"+index);
			HttpURLConnection con = (HttpURLConnection) obj.openConnection();
			con.setRequestProperty("User-Agent", "Mozilla/5.0 (Windows NT 6.1; WOW64) ...");
			con.setRequestProperty("Accept-Language", "en-US,en;q=0.5");

			con.setRequestMethod("GET");
			int responseCode = con.getResponseCode();
			String responseBody = CM.readResponseBody(con.getInputStream());

			if (responseCode != 200)
			{
				continue;
			}

			Parser parser;
			try
			{
				parser = new Parser(responseBody);

				// 获取a标签
				NodeFilter filter = new TagNameFilter("a");
				NodeList nodes = parser.extractAllNodesThatMatch(filter);

				if (nodes != null && nodes.size() != 0)
				{
					for (int i = 0; i < nodes.size(); i += 1)
					{
						// 转换成普通的tag标签
						Tag tag = (Tag) nodes.elementAt(i);

						String href = tag.getAttribute("href");

						if (href != null && href.equals("") == false)
						{
							if (href.indexOf("/" + blogName + "/article/details") >= 0 && href.indexOf("#comments") < 0)
							{
								href = "http://blog.csdn.net" + href;

								blogMap.put(href, href);
							}
						}
					}
				}
			}
			catch (ParserException e)
			{
				continue;
			}
		}
		
		return blogMap;
	}

```

获取代理IP
------

上面已经介绍了HTTP代理的原理，现在我们来获取代理IP。

在百度上搜索免费代理IP，可以出来很多整合了免费代理IP的网站，我们随便选择其中一个。

[http://www.xicidaili.com/wn/1](http://www.xicidaili.com/wn/1)

其中`1`是代理IP列表的页数。

![这里写图片描述](http://img.blog.csdn.net/20161105203335729)

我们只需要关注表中两个列`IP地址`和`端口`。有了这两个属性，我们就可以使用代理IP。

![这里写图片描述](http://img.blog.csdn.net/20161105203456388)

观察页面结构，可以看到IP和port的位置。

下面是获取IP和port的详细过程。

构造一个IP和port对象。

```

public class IpAndPort
{
	String ip;
	int port;

	public IpAndPort(String ip, int port)
	{
		super();
		this.ip = ip;
		this.port = port;
	}

	public String getIp()
	{
		return ip;
	}

	public void setIp(String ip)
	{
		this.ip = ip;
	}

	public int getPort()
	{
		return port;
	}

	public void setPort(int port)
	{
		this.port = port;
	}

	@Override
	public String toString()
	{
		return ip + ":" + port;
	}
}
```

获取一个页面上面的IP和port。

```
	/**
	 * 获取单个页面的ip和端口
	 * 
	 * @param htmlStr
	 * @param imgMaxNum
	 * @return
	 */
	public static List<IpAndPort> getIpList(String htmlStr, int imgMaxNum)
	{
		List<IpAndPort> imgSrcList = new ArrayList<IpAndPort>();
		Parser parser;
		try
		{
			parser = new Parser(htmlStr);
			// 获取tr标签
			NodeFilter filter = new TagNameFilter("tr");
			NodeList nodes = parser.extractAllNodesThatMatch(filter);

			if (nodes != null && nodes.size() != 0)
			{
				int imgNum = nodes.size() < imgMaxNum ? nodes.size() : imgMaxNum;
				for (int i = 1; i < imgNum; i++)
				{
					// 转换成普通的tag标签
					Tag tag = (Tag) nodes.elementAt(i);
					NodeList list = tag.getChildren();
					String ip = list.elementAt(3).toPlainTextString().trim();
					String port = list.elementAt(5).toPlainTextString().trim();
					imgSrcList.add(new IpAndPort(ip, Integer.parseInt(port)));
				}
				return imgSrcList;
			}
			else
			{
				return null;
			}
		}
		catch (ParserException e)
		{
			return null;
		}
	}

```

获取所有页面的IP和port。

```
	/**
	 * 获取所有页面的代理IP和port
	 * 
	 * @return
	 * @throws IOException
	 */
	public static List<IpAndPort> getTotalList() throws IOException
	{
		List<IpAndPort> totalList = new ArrayList<IpAndPort>();

		List<String> urlList = new ArrayList<String>();
		for (int i = 1; i <= 260; i++)
		{
			String strTemp = new String("http://www.xicidaili.com/wn/" + i);
			urlList.add(strTemp);
		}

		for (String url : urlList)
		{

			try
			{
				URL obj = new URL(url);
				HttpURLConnection con = (HttpURLConnection) obj.openConnection();
				con.setRequestProperty("User-Agent", "Mozilla/5.0 (Windows NT 6.1; WOW64) ...");
				con.setRequestProperty("Accept-Language", "en-US,en;q=0.5");

				con.setRequestMethod("GET");
				int responseCode = con.getResponseCode();
				String responseBody = CM.readResponseBody(con.getInputStream());

				if (responseCode == 200)
				{
					List<IpAndPort> list = getIpList(responseBody, 200);
					totalList.addAll(list);

					System.out.println("查找到网页 " + url + " 上的代理IP" + list.size() + "个，现在已经找到 " + totalList.size() + " 个");
				}
				else
				{
					System.out.println("网页 " + url + " 返回错误码为" + responseCode + "，将跳过，现在已经找到 " + totalList.size() + " 个");
				}
			}
			catch (Exception e)
			{
				System.out.println("出现错误了，或者被封了IP，停下来，换一下本地IP地址再重新运行程序吧。");
				continue;
			}
		}

		return totalList;
	}
```

验证代理IP是否有效
----------

上一步获取到了IP和port，但是我们并不知道这个代理IP是否真的如网站上面宣称的有效。这个时候我就需要自己去测试它是否有效。测试的方法也很简单。打开百度页面试一下。

```
	/**
	 * 测试代理是否有效
	 * 
	 * @param ip
	 * @param port
	 * @return
	 */
	public static boolean testIpEffective(String ip, int port)
	{
		try
		{
			Proxy proxy = new Proxy(Proxy.Type.HTTP, new InetSocketAddress(ip, port));
			URL obj = new URL("http://www.baidu.com");
			HttpURLConnection con = (HttpURLConnection) obj.openConnection(proxy);
			con.setRequestProperty("User-Agent", "Mozilla/5.0 (Windows NT 6.1; WOW64) ...");
			con.setRequestProperty("Accept-Language", "en-US,en;q=0.5");
			con.setConnectTimeout(8000);
			con.setReadTimeout(8000);

			con.setRequestMethod("GET");
			int responseCode = con.getResponseCode();
			if (responseCode == 200)
			{
				return true;
			}
			else
			{
				return false;
			}
		}
		catch (Exception e)
		{
			return false;
		}
	}
```

刷博客文章访问量
--------

代理IP也拿到了，也有效，那就可以开始进行最后一步了。

```
	/**
	 * 访问博客,解析博客标题和访问次数
	 * 
	 * @param ip
	 * @param port
	 */
	private void visitBlogLink(String ip, int port)
	{
		for (Map.Entry<String, String> entry : Test.blogMap.entrySet())
		{
			String url = entry.getKey();

			try
			{
				Proxy proxy = new Proxy(Proxy.Type.HTTP, new InetSocketAddress(ip, port));
				URL obj = new URL(url);
				HttpURLConnection con = (HttpURLConnection) obj.openConnection(proxy);
				con.setRequestProperty("User-Agent", "Mozilla/5.0 (Windows NT 6.1; WOW64) ...");
				con.setRequestProperty("Accept-Language", "en-US,en;q=0.5");

				con.setRequestMethod("GET");
				int responseCode = con.getResponseCode();
				String responseBody = CM.readResponseBody(con.getInputStream());
				if (responseCode == 200)
				{
					Parser parser;
					try
					{
						parser = new Parser(responseBody);
						// 获取标题标签
						NodeList titleNodes = parser.extractAllNodesThatMatch(new AndFilter(new TagNameFilter("a"), new HasAttributeFilter("href", url.replace(
								"http://blog.csdn.net", ""))));

						if (titleNodes != null && titleNodes.size() != 0)
						{
							// 转换成普通的tag标签
							Tag tag = (Tag) titleNodes.elementAt(0);
							System.out.print(ip + ":" + port + "\t\t" + tag.toPlainTextString().trim() + "\t\t");
						}
						Pattern p = Pattern.compile("title=\\\"阅读次数\\\">((.*?)人阅读)<");
						Matcher m = p.matcher(responseBody);
						while (m.find())
						{
							String b = m.group(1);
							System.out.println(b);
						}
					}
					catch (ParserException e)
					{
						continue;
					}
				}
			}
			catch (IOException e)
			{
				continue;
			}
		}
	}
```


----------


完整代码上传到这里了。[http://download.csdn.net/detail/gane_cheng/9674046](http://download.csdn.net/detail/gane_cheng/9674046)

GitHub也开源了此代码。

[https://github.com/ganecheng/BangTheAccessHits](https://github.com/ganecheng/BangTheAccessHits)

另外 [yixianfeng41](http://blog.csdn.net/yixianfeng41) 大神也开源了python版本的刷访问量的代码。

[https://github.com/xianfengyi/BrushPV](https://github.com/xianfengyi/BrushPV)

**CSDN应该如何应对**
=============

方法一 登录才能访问文章
---

像评论，点赞一样，需要登录才能执行操作，和账户绑定。如果访问量和账户绑定，再多的代理IP也没用，这是终极解决方案，但是会损失成千上万的用户，因此此方法无异于自杀。网站希望越来越多的用户，能不登录就不登录，打开就能用，因此不可能冒着丢失用户的代价去防止刷访问量。

方法二 建立代理IP黑名单数据库
----------------

既然根源是代理IP，那么就屏蔽这些代理IP的访问。但是这样会造成大量的误杀。如果因为一个人使用了代理IP刷访问量被屏蔽，其他正常使用此代理IP的人就会被连累。因此也不适合。

方法三 根据广告是否被播放的原则来屏蔽IP
---------------------

其实使不使用代理IP，网站也不关心，只要播放了广告，网站就有收入，因此如果根据广告是否被播放来屏蔽IP，那么使用代理IP，AdBlock，hosts等方法去广告的用户都将会被牵连。误伤更多。

方法四 收集证据关闭作者博客
--------

既然破坏规则就要承担后果，在证据充足的情况下，可以关闭作者的博客来应对。当然这是我最不愿意看到的，我坚信，技术的问题就要用技术去解决。


----------
小弟才疏学浅，能想到的方法只有这四个，如果你认为还有更好的方法可以解决刷访问量的问题，欢迎留言一起探讨。

**参考文献**
====

[http://berdy.iteye.com/blog/1547342](http://berdy.iteye.com/blog/1547342)